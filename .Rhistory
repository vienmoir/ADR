df = read.csv("ds_expanded.csv", stringsAsFactors = FALSE)
df = subset(df, is.na(deactivated) == TRUE)
setwd("~/")
df = read.csv("ds_expanded.csv", stringsAsFactors = FALSE)
df = subset(df, is.na(deactivated) == TRUE)
df = data.table(df)
uni = df[,.(members_count=max(members_count),cnt=sum(cnt)), by=.(id,name)]
uni = uni[order(-uni$cnt),]
library(data.table)
df = data.table(df)
uni = df[,.(members_count=max(members_count),cnt=sum(cnt)), by=.(id,name)]
uni = uni[order(-uni$cnt),]
library("vkR")
vkOAuth(6275578, 'groups')
setAccessToken(access_token = '27b04bd1ec4d433e5a00f484e3a130e5c22f170b1a8a62c4c44ef4c39256e71f848371338a7898c5c4659')
walls <- rbindlist(apply(grps, 1, function(x) {
wall <- getWallExecute(owner_id = -as.numeric(x[1]), count = 500, filter = 'all')
data.table(wall$posts$text,name = x[2])
}), fill = TRUE)
#3й вариант
grps = uni[1:50,]
walls <- rbindlist(apply(grps, 1, function(x) {
wall <- getWallExecute(owner_id = -as.numeric(x[1]), count = 500, filter = 'all')
data.table(wall$posts$text,name = x[2])
}), fill = TRUE)
#3й вариант
grps = uni[1:30,]
walls <- rbindlist(apply(grps, 1, function(x) {
wall <- getWallExecute(owner_id = -as.numeric(x[1]), count = 200, filter = 'all')
data.table(wall$posts$text,name = x[2])
}), fill = TRUE)
View(walls)
club <- 'datascience'
wall <- getWallExecute(domain = club, count = 10, filter = 'all')
View(wall)
View(walls)
grps = uni[1:30,]
walls <- rbindlist(apply(grps, 1, function(x) {
wall <- getWallExecute(owner_id = -as.numeric(x[1]), count = 200, filter = 'all')
wall$posts$date <- as.POSIXct(wall$posts$date, origin="1970-01-01", tz='Europe/Moscow')
data.table(wall$posts$text,wall$posts$date,name = x[2])
}), fill = TRUE)
View(walls)
View(walls)
walls[1,3]
walls[1,2]
k = strsplit(walls[1,2],' ')
walls$V2 = as.character(walls$V2)
strsplit(walls[1,2],' ')
strsplit(walls[1,2],' ')
walls[1,2]
typeof(walls[1,2])
typeof(walls[1,2][1])
walls[1,2]
walls[1,2][2]
walls[1,2][1]
walls[1,2][[1]]
typeof(walls$V2)
typeof(walls$V2[1])
typeof(walls[1,2])
walls$V2[2]
strsplit(walls$V2[2],' ')
df = read.csv("ds_expanded.csv", stringsAsFactors = FALSE)
df = subset(df, is.na(deactivated) == TRUE)
df = data.table(df)
library("vkR")
library(data.table)
df = data.table(df)
uni = df[,.(members_count=max(members_count),cnt=sum(cnt)), by=.(id,name)]
uni = uni[order(-uni$cnt),]
#3й вариант
grps = uni[1:30,]
walls <- rbindlist(apply(grps, 1, function(x) {
wall <- getWallExecute(owner_id = -as.numeric(x[1]), count = 500, filter = 'all')
wall$posts$date <- as.POSIXct(wall$posts$date, origin="1970-01-01", tz='Europe/Moscow')
data.table(wall$posts$text,wall$posts$date,name = x[2])
}), fill = TRUE)
vkOAuth(6275578, 'groups')
setAccessToken(access_token = '5f94d54411a09f9473c834c8306c9bb8f5eeaed560fc9ef97b346bc13043fd79341b65e4513b92f403515')
walls <- rbindlist(apply(grps, 1, function(x) {
wall <- getWallExecute(owner_id = -as.numeric(x[1]), count = 500, filter = 'all')
wall$posts$date <- as.POSIXct(wall$posts$date, origin="1970-01-01", tz='Europe/Moscow')
data.table(wall$posts$text,wall$posts$date,name = x[2])
}), fill = TRUE)
walls = aggregate(V1 ~ name, data = walls, FUN = paste)
for (i in 1:30) {
walls$V1[i] = gsub("\n", " ", walls$V1[i])
}
library(tm)
docs <- Corpus(VectorSource((walls$V1)))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeWords, stopwords("russian"))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
library(SnowballC)
docs <- tm_map(docs, stemDocument, language = "english")
docs <- tm_map(docs, stemDocument, language = "russian")
dtm <- DocumentTermMatrix(docs, control = list(weighting = weightTfIdf))
m = as.matrix(dtm)
distMatrix <- dist(m, method="euclidean")
so <- as.data.frame(as.matrix(distMatrix), row.names = walls$name, col.names = walls$name)
colnames(so) = walls$name
ds_dist = t(so["Data Science",])
ds_dist = data.frame(ds_dist[order(ds_dist[,1]),])
colnames(ds_dist) = "dist"
ds_dist$names = rownames(ds_dist)
a = ds_dist$dist[2:15]
a = as.matrix(a)
library(gplots)
heatmap.2(cbind(a,a), Rowv = FALSE, trace="n", Colv = NA,
dendrogram = "none", labCol = "", labRow = ds_dist$names[2:15], cexRow = 0.75,
margins =c(5,25), main = "Text distances")
setwd("~/GitHub/ADR")
all = read.csv("cleared_all_reviews.csv", stringsAsFactors = FALSE)
all$X = NULL
library(tm)
docs2 = Corpus(VectorSource((all$review[1])))
docs2$content
docs23[[1]]$content
docs2[[1]]$content
docs = docs2
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
docs[[1]]$content
replacePunctuation <- content_transformer(function(x) {return (gsub("[[:punct:]]"," ", x))})
docs <- Corpus(VectorSource((all$review[1])))
docs <- tm_map(docs, replacePunctuation)
docs[[1]]$content
docs <- tm_map(docs, stripWhitespace)
docs[[1]]$content
docs <- Corpus(VectorSource((all$review)))
replacePunctuation <- content_transformer(function(x) {return (gsub("[[:punct:]]"," ", x))})
docs <- tm_map(docs, replacePunctuation)
docs <- tm_map(docs, removeWords, stopwords("russian"))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, stemDocument, language = "russian")
# tf-idf
review_dtm_tfidf <- DocumentTermMatrix(docs, control = list(weighting = weightTfIdf))
#review_dtm_tfidf = removeSparseTerms(review_dtm_tfidf, 0.95)
review_dtm_tfidf
inspect(review_dtm_tfidf[1,1:100])
replacePunctuation2 <- content_transformer(function(x) {return (gsub("[^a-zA-Zа-яА-Я]+", " ", x))})
docs <- tm_map(docs, replacePunctuation2)
docs[[1]]$content
docs <- Corpus(VectorSource((all$review[1])))
docs <- tm_map(docs, replacePunctuation2)
docs[[1]]$content
docs <- Corpus(VectorSource((all$review[1])))
docs[[1]]$content
docs <- Corpus(VectorSource((all$review)))
docs <- tm_map(docs, replacePunctuation2)
docs <- tm_map(docs, removeWords, stopwords("russian"))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, stemDocument, language = "russian")
# tf-idf
review_dtm_tfidf <- DocumentTermMatrix(docs, control = list(weighting = weightTfIdf))
#review_dtm_tfidf = removeSparseTerms(review_dtm_tfidf, 0.95)
review_dtm_tfidf
inspect(review_dtm_tfidf[1,1:100])
inspect(review_dtm_tfidf[1,100:300])
docs <- Corpus(VectorSource((all$review[1])))
docs <- tm_map(docs, replacePunctuation2)
docs[[1]]$content
all$review[1]
View(all)
cs = read.csv("антибиотики.csv")
setwd("~/")
cs = read.csv("антибиотики.csv")
View(cs)
cs = NULL
docs <- Corpus(VectorSource((all$review)))
replacePunctuation <- content_transformer(function(x) {return (gsub("[^a-zA-Zа-яА-Я]+", " ", x))})
docs <- tm_map(docs, replacePunctuation)
docs <- tm_map(docs, removeWords, stopwords("russian"))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, stemDocument, language = "russian")
docs <- tm_map(docs, stemDocument, language = "english")
# tf-idf
review_dtm_tfidf <- DocumentTermMatrix(docs, control = list(weighting = weightTfIdf))
#review_dtm_tfidf = removeSparseTerms(review_dtm_tfidf, 0.95)
review_dtm_tfidf
View(review_dtm_tfidf)
a = colnames(review_dtm_tfidf)
a = a$hr
shadvlength <- lapply(a,nchar)
shadmaxind <- which.max(shadvlength) ## Maximum element
nchar(shadvector[shadmaxind])
nchar(a[shadmaxind])
a[shadmaxind]
# tokenize into tri-grams
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm.bigram = TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
# tokenize into tri-grams
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm.bigram = TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
inspect(tdm.bigram[1:10,1])
inspect(tdm.bigram[1:100,1])
tdm.bigram = TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer, weighting = weightTfIdf))
inspect(tdm.bigram[1:100,1])
inspect(tdm.bigram[100:200,1])
inspect(tdm.bigram[200:300,1])
freq = sort(rowSums(as.matrix(tdm.bigram)),decreasing = TRUE)
freq.df = data.frame(word=names(freq), freq=freq)
head(freq.df, 20)
inspect(tdm)
inspect(tdm.trigram)
FourTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
tdm.fourgram <- TermDocumentMatrix(docs, control = list(tokenize = FourTokenizer, weighting = weightTfIdf))
# most popular
freq = sort(rowSums(as.matrix(tdm.fourgram)),decreasing = TRUE)
inspect(tdm.fourgram)
setwd("~/GitHub/ADR")
all = read.csv("cleared_all_reviews.csv", stringsAsFactors = FALSE)
all$X = NULL
library(tm)
docs <- Corpus(VectorSource((all$review)))
replacePunctuation <- content_transformer(function(x) {return (gsub("[^a-zA-Zа-яА-Я]+", " ", x))})
docs <- tm_map(docs, replacePunctuation)
docs <- tm_map(docs, removeWords, stopwords("russian"))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
library(SnowballC)
docs <- tm_map(docs, stemDocument, language = "russian")
docs <- tm_map(docs, stemDocument, language = "english")
docs$content[1]
docs$content[[1]
]
docs$content[[1]]
docs$content[[3]]
